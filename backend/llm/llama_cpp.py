# CPP implementation - use CPU for highly quantized models - adjust to use Metal GPU on Mac
#   llama_model_cpp = Llama(
#    model_path="./models/rocket-3b.Q4_K_M.gguf",
#    n_gpu_layers=0  # Increase if you have GPU
#    )
